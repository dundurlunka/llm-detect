Да разцепим всеки текст на максимум 512 токена (или по-малко), за да улесним LSTM-а.
Да обучим модела върху определени теми, защото в обучаващите данни се делят на различни теми.
Да покажем коя с коя тема се бърка.
Да направим някакъв тип confusion matrix, където да показваме върху кой dataset е трениран модела и как се представя срещу всеки останал. Друг вариант е да се тренира върху един и да се показва срещу всички останали накуп.
Да натренираме един модел с всички данни
Да се намерят нови невиждани данни и да се тества модела от 5ти ред върху тях
Да се тестват досегашнтие модели с невижданите данни от 6ти ред

Да замразим някви слоеве, може с тва trainer да става
Освен файн тюнване, да видим какви резултати може да постигнем само с един fully connected layer, който ползва ембединга на последния токен от берт
Да ползваме ембедингите за всяка дума и да ги подаваме на някъв лстм + fully connected