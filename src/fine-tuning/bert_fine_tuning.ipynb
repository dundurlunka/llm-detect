{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioePof-Olcbe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zh5KhRZJYWrA"
      },
      "outputs": [],
      "source": [
        "from os import path, listdir, walk\n",
        "import datetime\n",
        "import json\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, brier_score_loss\n",
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wC_92TXMuiw"
      },
      "source": [
        "## Read data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YA4QQ1ndYsdy"
      },
      "outputs": [],
      "source": [
        "ENCODED_DATA_PATH = '/content/tokenized'\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_SIZE = 768\n",
        "TEST_SET_FRACTION = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qMbCdPMnMqV6"
      },
      "outputs": [],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCC5BNhkYZXj",
        "outputId": "f6a93f28-9253-4c2b-db2f-f5d61ec472b0"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_df = pd.read_pickle('/content/tokenized_texts.pkl')"
      ],
      "metadata": {
        "id": "4IXunaGQeF_d"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "models = ['gemini-pro', 'gpt-4-turbo-preview', 'gpt-3.5-turbo-0125', 'mistralai-mixtral-8x7b-instruct-v0.1', 'bigscience-bloomz-7b1', 'meta-llama-llama-2-7b-chat-hf', 'chavinlo-alpaca-13b']"
      ],
      "metadata": {
        "id": "2rQB2VV4K7g3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t29Vr3JDZ2FM"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmXCuPoPSqEN"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j42mFtoBSqEN"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the device using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(b_input_ids,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = y_hat.loss\n",
        "        train_loss += loss.item()\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    return train_loss / steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VorT_DFlSqEN"
      },
      "outputs": [],
      "source": [
        "def test_step(model, dataloader, loss_fn, device):\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    best_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    steps = 0\n",
        "\n",
        "    all_y_true = []\n",
        "    all_y_hat = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in dataloader:\n",
        "        steps = steps + 1\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.inference_mode():\n",
        "            y_hat = model(b_input_ids,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        loss = y_hat.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        # Move logits and labels to CPU if we are using GPU\n",
        "        logits = y_hat.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        # Calculate the accuracy for this batch of test sentences, and\n",
        "        # accumulate it over all batches.\n",
        "        total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "\n",
        "    return total_eval_loss / steps, total_eval_accuracy / steps\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XyVdPdafSqEO"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          train_dataloader,\n",
        "          test_dataloader,\n",
        "          optimizer,\n",
        "          loss_fn,\n",
        "          epochs,\n",
        "          device):\n",
        "\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "    }\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        start_time = timer()\n",
        "        train_loss = train_step(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "        )\n",
        "        end_time = timer()\n",
        "\n",
        "        test_loss, test_acc = test_step(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f} | \"\n",
        "            f\"time: {(end_time-start_time):.4f}\"\n",
        "        )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "idDVDAVaSqEO"
      },
      "outputs": [],
      "source": [
        "def get_tensor_dataset_from_df(df):\n",
        "    input_ids = torch.cat(list(df['input_ids']), dim=0)\n",
        "    attention_masks = torch.cat(list(df['attention_mask']), dim=0)\n",
        "    labels = torch.tensor(list(df['labels']))\n",
        "\n",
        "    return TensorDataset(input_ids, attention_masks, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eDHY-IuPSqEO"
      },
      "outputs": [],
      "source": [
        "def test_against_all(model, trained_llm_name, human_test_df, llm_df, loss_fn, device):\n",
        "    all_results = []\n",
        "\n",
        "    for dataset_name in models:\n",
        "        curr_llm_df = llm_df.loc[llm_df['llm_name'] == dataset_name]\n",
        "        test_df = pd.concat([human_test_df, curr_llm_df], ignore_index=True)\n",
        "\n",
        "      #  train_dataset = get_tensor_dataset_from_df()\n",
        "        test_dataset = get_tensor_dataset_from_df(test_df)\n",
        "\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "        results = test_step(\n",
        "            model,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        all_results.append({\n",
        "            dataset_name: results\n",
        "        })\n",
        "\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eFhEojaMSqEO"
      },
      "outputs": [],
      "source": [
        "human_train_df, human_test_df = train_test_split(tokenized_df.loc[tokenized_df['llm_name'] == 'human'], test_size=TEST_SET_FRACTION, random_state=69)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yw3nhvISqEO",
        "outputId": "43aaa7cf-9dee-4407-9a53-0458c9899b91"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for gemini-pro...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:29<04:28, 89.50s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.3005 | test_loss: 0.2061 | test_acc: 0.9391 | time: 77.0749\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:58<02:58, 89.19s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.0717 | test_loss: 0.0452 | test_acc: 0.9797 | time: 76.3648\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:27<01:29, 89.10s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0413 | test_loss: 0.0632 | test_acc: 0.9859 | time: 76.2754\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:56<00:00, 89.12s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0093 | test_loss: 0.0786 | test_acc: 0.9828 | time: 76.3151\n",
            "Finished training model for gemini-pro\n",
            "Testing against all for gemini-pro...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for gemini-pro\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for gpt-4-turbo-preview...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:29<04:27, 89.07s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.2640 | test_loss: 0.0455 | test_acc: 0.9859 | time: 76.3823\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:58<02:58, 89.22s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.0601 | test_loss: 0.0216 | test_acc: 0.9953 | time: 76.5965\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:27<01:29, 89.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0126 | test_loss: 0.0372 | test_acc: 0.9922 | time: 76.4368\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:56<00:00, 89.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0018 | test_loss: 0.0487 | test_acc: 0.9906 | time: 76.1820\n",
            "Finished training model for gpt-4-turbo-preview\n",
            "Testing against all for gpt-4-turbo-preview...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for gpt-4-turbo-preview\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for gpt-3.5-turbo-0125...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:28<04:25, 88.60s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.2267 | test_loss: 0.0846 | test_acc: 0.9781 | time: 75.9354\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:57<02:57, 88.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.0386 | test_loss: 0.0470 | test_acc: 0.9906 | time: 76.2469\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:26<01:28, 88.88s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0340 | test_loss: 0.0383 | test_acc: 0.9906 | time: 76.2986\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:55<00:00, 88.84s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0127 | test_loss: 0.0243 | test_acc: 0.9953 | time: 76.1378\n",
            "Finished training model for gpt-3.5-turbo-0125\n",
            "Testing against all for gpt-3.5-turbo-0125...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for gpt-3.5-turbo-0125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for mistralai-mixtral-8x7b-instruct-v0.1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:28<04:26, 88.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.3826 | test_loss: 0.1798 | test_acc: 0.9281 | time: 76.2420\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:58<02:58, 89.16s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.1745 | test_loss: 0.3702 | test_acc: 0.8859 | time: 76.5626\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:27<01:29, 89.08s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0631 | test_loss: 0.6499 | test_acc: 0.8297 | time: 76.3304\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:56<00:00, 89.06s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0452 | test_loss: 0.1746 | test_acc: 0.9641 | time: 76.3045\n",
            "Finished training model for mistralai-mixtral-8x7b-instruct-v0.1\n",
            "Testing against all for mistralai-mixtral-8x7b-instruct-v0.1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for mistralai-mixtral-8x7b-instruct-v0.1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for bigscience-bloomz-7b1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:27<04:23, 87.68s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.3070 | test_loss: 0.1165 | test_acc: 0.9563 | time: 75.0590\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:55<02:55, 87.81s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.1400 | test_loss: 0.1444 | test_acc: 0.9516 | time: 75.2651\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:23<01:27, 87.85s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0387 | test_loss: 0.2588 | test_acc: 0.9469 | time: 75.3058\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:50<00:00, 87.71s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0299 | test_loss: 0.2928 | test_acc: 0.9453 | time: 74.8301\n",
            "Finished training model for bigscience-bloomz-7b1\n",
            "Testing against all for bigscience-bloomz-7b1...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for bigscience-bloomz-7b1\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for meta-llama-llama-2-7b-chat-hf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:28<04:26, 88.86s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.3585 | test_loss: 0.4650 | test_acc: 0.8500 | time: 76.1276\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:57<02:57, 88.94s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.0634 | test_loss: 0.0970 | test_acc: 0.9688 | time: 76.3205\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:26<01:28, 88.96s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0461 | test_loss: 0.0460 | test_acc: 0.9844 | time: 76.2867\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:55<00:00, 88.95s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0119 | test_loss: 0.1202 | test_acc: 0.9750 | time: 76.2871\n",
            "Finished training model for meta-llama-llama-2-7b-chat-hf\n",
            "Testing against all for meta-llama-llama-2-7b-chat-hf...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for meta-llama-llama-2-7b-chat-hf\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for chavinlo-alpaca-13b...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 25%|██▌       | 1/4 [01:26<04:19, 86.39s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.2020 | test_loss: 0.0719 | test_acc: 0.9797 | time: 73.7970\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 50%|█████     | 2/4 [02:52<02:52, 86.18s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.0262 | test_loss: 0.0823 | test_acc: 0.9812 | time: 73.5425\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 75%|███████▌  | 3/4 [04:18<01:26, 86.13s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 3 | train_loss: 0.0072 | test_loss: 0.0490 | test_acc: 0.9875 | time: 73.5847\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4/4 [05:44<00:00, 86.14s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 4 | train_loss: 0.0006 | test_loss: 0.0461 | test_acc: 0.9922 | time: 73.5868\n",
            "Finished training model for chavinlo-alpaca-13b\n",
            "Testing against all for chavinlo-alpaca-13b...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Finished testing against all for chavinlo-alpaca-13b\n"
          ]
        }
      ],
      "source": [
        "final_results = []\n",
        "\n",
        "for llm_name in models:\n",
        "    llm_df = tokenized_df.loc[tokenized_df['llm_name'] == llm_name]\n",
        "\n",
        "    llm_train_df, llm_test_df = train_test_split(llm_df, test_size=TEST_SET_FRACTION, random_state=69)\n",
        "    train_df = pd.concat([human_train_df, llm_train_df], ignore_index=True)\n",
        "    test_df = pd.concat([human_test_df, llm_test_df], ignore_index=True)\n",
        "\n",
        "    train_dataset = get_tensor_dataset_from_df(train_df)\n",
        "    test_dataset = get_tensor_dataset_from_df(test_df)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "    # linear classification layer on top.\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-cased\",\n",
        "        num_labels = 2,\n",
        "        output_attentions = False, # Whether the model returns attentions weights.\n",
        "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "\n",
        "    if device == \"cuda:0\":\n",
        "      model = model.cuda()\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr =5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "    epochs = 3\n",
        "\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    print(f'Training model for {llm_name}...')\n",
        "\n",
        "    current_results = train(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        test_dataloader,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        epochs=4,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f'Finished training model for {llm_name}')\n",
        "\n",
        "    print(f'Testing against all for {llm_name}...')\n",
        "\n",
        "    results_against_all = test_against_all(\n",
        "        model,\n",
        "        llm_name,\n",
        "        human_test_df,\n",
        "        tokenized_df,\n",
        "        loss_fn,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    total_results = {\n",
        "        'results_against_itself': current_results,\n",
        "        'results_against_all': results_against_all\n",
        "    }\n",
        "\n",
        "    final_results.append(total_results)\n",
        "\n",
        "    with open(f'data-{llm_name}.json', 'w') as f:\n",
        "      json.dump(total_results, f)\n",
        "\n",
        "    print(f'Finished testing against all for {llm_name}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open('topic-data.json', 'w') as f:\n",
        "    json.dump(final_results, f)"
      ],
      "metadata": {
        "id": "Wuf3A5KZg7uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}