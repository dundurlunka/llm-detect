{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioePof-Olcbe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zh5KhRZJYWrA"
      },
      "outputs": [],
      "source": [
        "from os import path, listdir, walk\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import f1_score, roc_curve, auc, brier_score_loss\n",
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wC_92TXMuiw"
      },
      "source": [
        "## Read data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YA4QQ1ndYsdy"
      },
      "outputs": [],
      "source": [
        "ENCODED_DATA_PATH = '/content/tokenized'\n",
        "RAW_DATA_PATH = '/content/text'\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_SIZE = 512\n",
        "TEST_SET_FRACTION = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMbCdPMnMqV6",
        "outputId": "d9c75f95-7d18-492e-b0dd-5e3d24553180"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCC5BNhkYZXj",
        "outputId": "ef3ca3ce-d582-4f78-c4ce-cafce4ccc567"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ],
      "metadata": {
        "id": "w46qIAybZOCQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "raw_df = pd.DataFrame(columns=['llm_name', 'text', 'is_llm'])\n",
        "\n",
        "dir_path, dir_names, file_names = next(walk(RAW_DATA_PATH))\n",
        "\n",
        "#print(file_names)\n",
        "for file_name in file_names:\n",
        "    temp_df = pd.read_json(path_or_buf=f'{RAW_DATA_PATH}/{file_name}', lines=True)\n",
        "    temp_df['is_llm'] = 0 if file_name.startswith('human') else 1\n",
        "    temp_df['llm_name'] = path.splitext(file_name)[0]\n",
        "    temp_df.drop(labels=['id'], inplace=True, axis='columns')\n",
        "    raw_df = pd.concat([raw_df, temp_df], ignore_index=True)"
      ],
      "metadata": {
        "id": "6ln23PbSZZR_"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_df = pd.read_pickle('/content/tokenized_texts.pkl')\n"
      ],
      "metadata": {
        "id": "4IXunaGQeF_d"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "VA_rSKvfZqxK"
      },
      "outputs": [],
      "source": [
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t29Vr3JDZ2FM"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "xmXCuPoPSqEN"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(preds, labels):\n",
        "    pred_flat = np.argmax(preds).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def calculate_f1(y_true, y_hat):\n",
        "    y_pred = torch.sigmoid(y_hat).round()\n",
        "    return f1_score(y_true, y_pred)\n",
        "\n",
        "def calculate_brier(y_true, y_hat):\n",
        "    y_prob = torch.sigmoid(y_hat)\n",
        "    return brier_score_loss(y_true, y_prob)\n",
        "\n",
        "def calculate_auc(y_true, y_hat):\n",
        "    y_prob = torch.sigmoid(y_hat)\n",
        "\n",
        "    false_positive_rates, true_positive_rates, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(false_positive_rates, true_positive_rates)\n",
        "\n",
        "    return roc_auc, false_positive_rates, true_positive_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "j42mFtoBSqEN"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    train_loss, train_acc = 0, 0\n",
        "    steps = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the device using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(b_input_ids,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = y_hat.loss\n",
        "        train_loss += loss.item()\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "        train_acc += flat_accuracy(y_hat, b_labels)\n",
        "        steps += 1\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    return train_loss / steps, train_acc / steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "VorT_DFlSqEN"
      },
      "outputs": [],
      "source": [
        "def test_step(model, dataloader, loss_fn, device):\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    best_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    steps = 0\n",
        "\n",
        "    all_y_true = []\n",
        "    all_y_hat = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in dataloader:\n",
        "        steps += 1\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.inference_mode():\n",
        "            y_hat= model(b_input_ids,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        loss = y_hat.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        # Move logits and labels to CPU if we are using GPU\n",
        "        logits = y_hat.logits\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "        all_y_true.extend(b_labels)\n",
        "        all_y_hat.extend(y_hat)\n",
        "\n",
        "    all_y_true = torch.FloatTensor(all_y_true)\n",
        "    all_y_hat = torch.FloatTensor(all_y_hat)\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    test_accuracy = flat_accuracy(all_y_hat, all_y_true)\n",
        "    test_f1 = calculate_f1(all_y_true, all_y_hat)\n",
        "    test_brier = calculate_brier(all_y_true, all_y_hat)\n",
        "    test_auc_tuple = calculate_auc(all_y_true, all_y_hat)\n",
        "\n",
        "    return total_eval_loss / steps, test_accuracy, test_f1, test_brier, test_auc_tuple\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "XyVdPdafSqEO"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          train_dataloader,\n",
        "          test_dataloader,\n",
        "          optimizer,\n",
        "          loss_fn,\n",
        "          epochs,\n",
        "          device):\n",
        "\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "        \"train_acc\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"test_f1\": [],\n",
        "        \"test_brier\": [],\n",
        "        \"test_auc_tuple\": []\n",
        "    }\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        start_time = timer()\n",
        "        train_loss, train_acc = train_step(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "        )\n",
        "        end_time = timer()\n",
        "\n",
        "        test_loss, test_acc, test_f1, test_brier, test_auc_tuple = test_step(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"train_acc\"].append(train_acc)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "        results[\"test_f1\"].append(test_f1)\n",
        "        results[\"test_brier\"].append(test_brier),\n",
        "        results[\"test_auc_tuple\"].append(test_auc_tuple)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"train_acc: {train_acc:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f} | \"\n",
        "            f\"test_f1: {test_f1:.4f} | \"\n",
        "            f\"test_brier: {test_brier:.4f} | \"\n",
        "            f\"time: {(end_time-start_time):.4f}\"\n",
        "        )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "idDVDAVaSqEO"
      },
      "outputs": [],
      "source": [
        "def get_tensor_dataset_from_df(df):\n",
        "    input_ids = torch.cat(list(df['input_ids']), dim=0)\n",
        "    attention_masks = torch.cat(list(df['attention_mask']), dim=0)\n",
        "    labels = torch.tensor(list(df['labels']))\n",
        "\n",
        "    return TensorDataset(input_ids, attention_masks, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "eDHY-IuPSqEO"
      },
      "outputs": [],
      "source": [
        "def test_against_all(model, trained_llm_name, human_test_df, llm_df, loss_fn, device):\n",
        "    all_results = []\n",
        "\n",
        "    for dataset_name in llm_df['llm_name'].unique():\n",
        "        curr_llm_df = llm_df.loc[llm_df['llm_name'] == dataset_name]\n",
        "        test_df = pd.concat([human_test_df, curr_llm_df], ignore_index=True)\n",
        "\n",
        "      #  train_dataset = get_tensor_dataset_from_df()\n",
        "        test_dataset = get_tensor_dataset_from_df(test_df)\n",
        "\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "        results = test_step(\n",
        "            model,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        all_results.append({\n",
        "            dataset_name: results\n",
        "        })\n",
        "\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "eFhEojaMSqEO"
      },
      "outputs": [],
      "source": [
        "human_train_df, human_test_df = train_test_split(tokenized_df.loc[tokenized_df['llm_name'] == 'human'], test_size=TEST_SET_FRACTION, random_state=69)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = tokenized_df.head(1)\n",
        "print(t['input_ids'].shape, t['attention_mask'].shape, t['labels'].shape)\n",
        "t1 = get_tensor_dataset_from_df(t)\n",
        "t1"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wuf3A5KZg7uz",
        "outputId": "ba5057ae-3be6-4557-d441-a7cbf44f365d"
      },
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1,) (1,) (1,)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch.utils.data.dataset.TensorDataset at 0x7cbc043baa40>"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 505
        },
        "id": "7Yw3nhvISqEO",
        "outputId": "3e22747f-4814-4af9-db66-7eebcbfe9a3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "/usr/local/lib/python3.10/dist-packages/transformers/optimization.py:588: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for mistralai-mistral-7b-instruct-v0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "  0%|          | 0/10 [01:25<?, ?it/s]\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "too many dimensions 'str'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-18-ae404dfa854d>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'Training model for {llm_name}...'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     current_results = train(\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mtrain_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-13-132e17975372>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(model, train_dataloader, test_dataloader, optimizer, loss_fn, epochs, device)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mend_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtimer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         test_loss, test_acc, test_f1, test_brier, test_auc_tuple = test_step(\n\u001b[0m\u001b[1;32m     34\u001b[0m             \u001b[0mmodel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m             \u001b[0mdataloader\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtest_dataloader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-12-125b37e8ed6e>\u001b[0m in \u001b[0;36mtest_step\u001b[0;34m(model, dataloader, loss_fn, device)\u001b[0m\n\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mall_y_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_y_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 37\u001b[0;31m     \u001b[0mall_y_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_y_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     38\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;31m# Calculate the average loss over all of the batches.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: too many dimensions 'str'"
          ]
        }
      ],
      "source": [
        "final_results = []\n",
        "\n",
        "for llm_name in tokenized_df['llm_name']:\n",
        "    llm_df = tokenized_df.loc[tokenized_df['llm_name'] == llm_name]\n",
        "\n",
        "    llm_train_df, llm_test_df = train_test_split(llm_df, test_size=TEST_SET_FRACTION, random_state=69)\n",
        "    train_df = pd.concat([human_train_df, llm_train_df], ignore_index=True)\n",
        "    test_df = pd.concat([human_test_df, llm_test_df], ignore_index=True)\n",
        "\n",
        "    train_dataset = get_tensor_dataset_from_df(train_df)\n",
        "    test_dataset = get_tensor_dataset_from_df(test_df)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        drop_last=False,\n",
        "    )\n",
        "\n",
        "    # Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "    # linear classification layer on top.\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-cased\",\n",
        "        num_labels = 2, # The number of output labels--2 for binary classification.\n",
        "        output_attentions = False, # Whether the model returns attentions weights.\n",
        "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "\n",
        "    if device == \"cuda:0\":\n",
        "      model = model.cuda()\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "    epochs = 3\n",
        "\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    print(f'Training model for {llm_name}...')\n",
        "\n",
        "    current_results = train(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        test_dataloader,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        epochs=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f'Finished training model for {llm_name}')\n",
        "\n",
        "    print(f'Testing against all for {llm_name}...')\n",
        "\n",
        "    results_against_all = test_against_all(\n",
        "        model,\n",
        "        llm_name,\n",
        "        human_test_df,\n",
        "        tokenized_df,\n",
        "        loss_fn,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    final_results.append({\n",
        "        'results_against_itself': current_results,\n",
        "        'results_against_all': results_against_all\n",
        "    })\n",
        "\n",
        "    print(f'Finished testing against all for {llm_name}')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}