{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioePof-Olcbe"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "Zh5KhRZJYWrA"
      },
      "outputs": [],
      "source": [
        "from os import path, listdir, walk\n",
        "import datetime\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from timeit import default_timer as timer\n",
        "from tqdm import tqdm\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, f1_score, roc_curve, auc, brier_score_loss\n",
        "from transformers import DistilBertForSequenceClassification, AdamW, DistilBertTokenizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wC_92TXMuiw"
      },
      "source": [
        "## Read data\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "YA4QQ1ndYsdy"
      },
      "outputs": [],
      "source": [
        "ENCODED_DATA_PATH = '/content/tokenized'\n",
        "RAW_DATA_PATH = '/content/text'\n",
        "BATCH_SIZE = 32\n",
        "MAX_SEQ_SIZE = 768\n",
        "TEST_SET_FRACTION = 0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qMbCdPMnMqV6",
        "outputId": "4d794abe-fa50-4771-a46e-c581131fa287"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_token.py:89: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "tokenizer = DistilBertTokenizer.from_pretrained('distilbert-base-cased')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jCC5BNhkYZXj",
        "outputId": "6a21f775-3093-4075-843d-dc027c3c58a9"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "device(type='cuda', index=0)"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ],
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "device"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tokenized_df = pd.read_pickle('/content/tokenized_texts.pkl')"
      ],
      "metadata": {
        "id": "4IXunaGQeF_d"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "t29Vr3JDZ2FM"
      },
      "outputs": [],
      "source": [
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "xmXCuPoPSqEN"
      },
      "outputs": [],
      "source": [
        "# Function to calculate the accuracy of our predictions vs labels\n",
        "def flat_accuracy(y_true, y_hat):\n",
        "    y_pred = torch.sigmoid(y_hat).round()\n",
        "    return accuracy_score(y_true, y_pred)\n",
        "\n",
        "def calculate_f1(y_true, y_hat):\n",
        "    y_pred = torch.sigmoid(y_hat).round()\n",
        "    return f1_score(y_true, y_pred)\n",
        "\n",
        "def calculate_brier(y_true, y_hat):\n",
        "    y_prob = torch.sigmoid(y_hat)\n",
        "    return brier_score_loss(y_true, y_prob)\n",
        "\n",
        "def calculate_auc(y_true, y_hat):\n",
        "    y_prob = torch.sigmoid(y_hat)\n",
        "\n",
        "    false_positive_rates, true_positive_rates, _ = roc_curve(y_true, y_prob)\n",
        "    roc_auc = auc(false_positive_rates, true_positive_rates)\n",
        "\n",
        "    return roc_auc, false_positive_rates, true_positive_rates"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "j42mFtoBSqEN"
      },
      "outputs": [],
      "source": [
        "def train_step(model, dataloader, loss_fn, optimizer, device):\n",
        "\n",
        "    model.train()\n",
        "    train_loss = 0\n",
        "    steps = 0\n",
        "\n",
        "    for batch in dataloader:\n",
        "        # Unpack this training batch from our dataloader.\n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the device using the\n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids\n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        optimizer.zero_grad()\n",
        "        y_hat = model(b_input_ids,\n",
        "                             attention_mask=b_input_mask,\n",
        "                             labels=b_labels)\n",
        "        loss = y_hat.loss\n",
        "        train_loss += loss.item()\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "        steps += 1\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    return train_loss / steps"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "VorT_DFlSqEN"
      },
      "outputs": [],
      "source": [
        "def test_step(model, dataloader, loss_fn, device):\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "    # Tracking variables\n",
        "    total_eval_accuracy = 0\n",
        "    best_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    nb_eval_steps = 0\n",
        "    steps = 0\n",
        "\n",
        "    all_y_true = []\n",
        "    all_y_hat = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in dataloader:\n",
        "        steps += 1\n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.inference_mode():\n",
        "            y_hat = model(b_input_ids,\n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)\n",
        "        loss = y_hat.loss\n",
        "        total_eval_loss += loss.item()\n",
        "        # Move logits and labels to CPU if we are using GPU\n",
        "        logits = y_hat.logits.detach().cpu()\n",
        "        label_ids = b_labels.detach().cpu().reshape(-1,1)\n",
        "        logits = torch.max(logits, dim=1, keepdim=True).values\n",
        "        all_y_true.extend(label_ids)\n",
        "        all_y_hat.extend(logits)\n",
        "\n",
        "    all_y_true = torch.cat(all_y_true, dim=0)\n",
        "    all_y_hat = torch.cat(all_y_hat, dim=0)\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    test_accuracy = flat_accuracy(all_y_true, all_y_hat)\n",
        "    test_f1 = calculate_f1(all_y_true, all_y_hat)\n",
        "    test_brier = calculate_brier(all_y_true, all_y_hat)\n",
        "    test_auc_tuple = calculate_auc(all_y_true, all_y_hat)\n",
        "\n",
        "    return total_eval_loss / steps, test_accuracy, test_f1, test_brier, test_auc_tuple\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "XyVdPdafSqEO"
      },
      "outputs": [],
      "source": [
        "def train(model,\n",
        "          train_dataloader,\n",
        "          test_dataloader,\n",
        "          optimizer,\n",
        "          loss_fn,\n",
        "          epochs,\n",
        "          device):\n",
        "\n",
        "    results = {\n",
        "        \"train_loss\": [],\n",
        "        \"test_loss\": [],\n",
        "        \"test_acc\": [],\n",
        "        \"test_f1\": [],\n",
        "        \"test_brier\": [],\n",
        "        \"test_auc_tuple\": []\n",
        "    }\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    for epoch in tqdm(range(epochs)):\n",
        "\n",
        "        start_time = timer()\n",
        "        train_loss = train_step(\n",
        "            model=model,\n",
        "            dataloader=train_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            optimizer=optimizer,\n",
        "            device=device,\n",
        "        )\n",
        "        end_time = timer()\n",
        "\n",
        "        test_loss, test_acc, test_f1, test_brier, test_auc_tuple = test_step(\n",
        "            model=model,\n",
        "            dataloader=test_dataloader,\n",
        "            loss_fn=loss_fn,\n",
        "            device=device,\n",
        "        )\n",
        "\n",
        "        results[\"train_loss\"].append(train_loss)\n",
        "        results[\"test_loss\"].append(test_loss)\n",
        "        results[\"test_acc\"].append(test_acc)\n",
        "        results[\"test_f1\"].append(test_f1)\n",
        "        results[\"test_brier\"].append(test_brier),\n",
        "        results[\"test_auc_tuple\"].append(test_auc_tuple)\n",
        "\n",
        "        print(\n",
        "            f\"Epoch: {epoch+1} | \"\n",
        "            f\"train_loss: {train_loss:.4f} | \"\n",
        "            f\"test_loss: {test_loss:.4f} | \"\n",
        "            f\"test_acc: {test_acc:.4f} | \"\n",
        "            f\"test_f1: {test_f1:.4f} | \"\n",
        "            f\"test_brier: {test_brier:.4f} | \"\n",
        "            f\"time: {(end_time-start_time):.4f}\"\n",
        "        )\n",
        "\n",
        "    return results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "idDVDAVaSqEO"
      },
      "outputs": [],
      "source": [
        "def get_tensor_dataset_from_df(df):\n",
        "    input_ids = torch.cat(list(df['input_ids']), dim=0)\n",
        "    attention_masks = torch.cat(list(df['attention_mask']), dim=0)\n",
        "    labels = torch.tensor(list(df['labels']))\n",
        "\n",
        "    return TensorDataset(input_ids, attention_masks, labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "eDHY-IuPSqEO"
      },
      "outputs": [],
      "source": [
        "def test_against_all(model, trained_llm_name, human_test_df, llm_df, loss_fn, device):\n",
        "    all_results = []\n",
        "\n",
        "    for dataset_name in llm_df['llm_name'].unique():\n",
        "        curr_llm_df = llm_df.loc[llm_df['llm_name'] == dataset_name]\n",
        "        test_df = pd.concat([human_test_df, curr_llm_df], ignore_index=True)\n",
        "\n",
        "      #  train_dataset = get_tensor_dataset_from_df()\n",
        "        test_dataset = get_tensor_dataset_from_df(test_df)\n",
        "\n",
        "        test_dataloader = DataLoader(\n",
        "            test_dataset,\n",
        "            batch_size=BATCH_SIZE,\n",
        "            shuffle=False,\n",
        "            drop_last=True,\n",
        "        )\n",
        "\n",
        "        results = test_step(\n",
        "            model,\n",
        "            test_dataloader,\n",
        "            loss_fn,\n",
        "            device\n",
        "        )\n",
        "\n",
        "        all_results.append({\n",
        "            dataset_name: results\n",
        "        })\n",
        "\n",
        "    return all_results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "eFhEojaMSqEO"
      },
      "outputs": [],
      "source": [
        "human_train_df, human_test_df = train_test_split(tokenized_df.loc[tokenized_df['llm_name'] == 'human'], test_size=TEST_SET_FRACTION, random_state=69)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7Yw3nhvISqEO",
        "outputId": "33556322-25fc-4ac4-ad3d-718f9284a64b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight', 'pre_classifier.bias', 'pre_classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training model for mistralai-mistral-7b-instruct-v0.2...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 10%|█         | 1/10 [01:18<11:45, 78.42s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 1 | train_loss: 0.3405 | test_loss: 0.3316 | test_acc: 0.5016 | test_f1: 0.6624 | test_brier: 0.3734 | time: 67.2572\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r 20%|██        | 2/10 [02:39<10:39, 79.98s/it]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch: 2 | train_loss: 0.1561 | test_loss: 0.5861 | test_acc: 0.4953 | test_f1: 0.6596 | test_brier: 0.4346 | time: 69.5310\n"
          ]
        }
      ],
      "source": [
        "final_results = []\n",
        "\n",
        "for llm_name in tokenized_df['llm_name']:\n",
        "    llm_df = tokenized_df.loc[tokenized_df['llm_name'] == llm_name]\n",
        "\n",
        "    llm_train_df, llm_test_df = train_test_split(llm_df, test_size=TEST_SET_FRACTION, random_state=69)\n",
        "    train_df = pd.concat([human_train_df, llm_train_df], ignore_index=True)\n",
        "    test_df = pd.concat([human_test_df, llm_test_df], ignore_index=True)\n",
        "\n",
        "    train_dataset = get_tensor_dataset_from_df(train_df)\n",
        "    test_dataset = get_tensor_dataset_from_df(test_df)\n",
        "\n",
        "    train_dataloader = DataLoader(\n",
        "        train_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=True,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    test_dataloader = DataLoader(\n",
        "        test_dataset,\n",
        "        batch_size=BATCH_SIZE,\n",
        "        shuffle=False,\n",
        "        drop_last=True,\n",
        "    )\n",
        "\n",
        "    # Load BertForSequenceClassification, the pretrained BERT model with a single\n",
        "    # linear classification layer on top.\n",
        "    model = DistilBertForSequenceClassification.from_pretrained(\n",
        "        \"distilbert-base-cased\",\n",
        "        output_attentions = False, # Whether the model returns attentions weights.\n",
        "        output_hidden_states = False, # Whether the model returns all hidden-states.\n",
        "    )\n",
        "\n",
        "    if device == \"cuda:0\":\n",
        "      model = model.cuda()\n",
        "\n",
        "    model = model.to(device)\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                  lr = 5e-5, # args.learning_rate - default is 5e-5, our notebook had 2e-5\n",
        "                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n",
        "                )\n",
        "\n",
        "    epochs = 3\n",
        "\n",
        "    loss_fn = torch.nn.BCEWithLogitsLoss()\n",
        "\n",
        "    print(f'Training model for {llm_name}...')\n",
        "\n",
        "    current_results = train(\n",
        "        model,\n",
        "        train_dataloader,\n",
        "        test_dataloader,\n",
        "        optimizer,\n",
        "        loss_fn,\n",
        "        epochs=10,\n",
        "        device=device\n",
        "    )\n",
        "\n",
        "    print(f'Finished training model for {llm_name}')\n",
        "\n",
        "    print(f'Testing against all for {llm_name}...')\n",
        "\n",
        "    results_against_all = test_against_all(\n",
        "        model,\n",
        "        llm_name,\n",
        "        human_test_df,\n",
        "        tokenized_df,\n",
        "        loss_fn,\n",
        "        device\n",
        "    )\n",
        "\n",
        "    final_results.append({\n",
        "        'results_against_itself': current_results,\n",
        "        'results_against_all': results_against_all\n",
        "    })\n",
        "\n",
        "    print(f'Finished testing against all for {llm_name}')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "t = tokenized_df.head(1)\n",
        "print(t['input_ids'].shape, t['attention_mask'].shape, t['labels'].shape)\n",
        "t1 = get_tensor_dataset_from_df(t)\n",
        "t1"
      ],
      "metadata": {
        "id": "Wuf3A5KZg7uz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}