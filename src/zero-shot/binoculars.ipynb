{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHLeW1HMssyV"
      },
      "source": [
        "# Binoculars experiments"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6C6WyiFhtDsx"
      },
      "source": [
        "## Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UddLoR_GujmD",
        "outputId": "cf2b3d72-f4b7-49bf-f34d-f02356f8a06b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting accelerate\n",
            "  Downloading accelerate-0.32.0-py3-none-any.whl (314 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m314.0/314.0 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting bitsandbytes\n",
            "  Downloading bitsandbytes-0.43.1-py3-none-manylinux_2_24_x86_64.whl (119.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m119.8/119.8 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy<2.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate) (1.25.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (24.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate) (5.9.5)\n",
            "Requirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate) (6.0.1)\n",
            "Requirement already satisfied: torch>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from accelerate) (2.3.0+cu121)\n",
            "Requirement already satisfied: huggingface-hub in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.23.4)\n",
            "Requirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from accelerate) (0.4.3)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.15.4)\n",
            "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (4.12.2)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.20.5 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nccl_cu12-2.20.5-py3-none-manylinux2014_x86_64.whl (176.2 MB)\n",
            "Collecting nvidia-nvtx-cu12==12.1.105 (from torch>=1.10.0->accelerate)\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Requirement already satisfied: triton==2.3.0 in /usr/local/lib/python3.10/dist-packages (from torch>=1.10.0->accelerate) (2.3.0)\n",
            "Collecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.10.0->accelerate)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.82-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m48.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (2.31.0)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub->accelerate) (4.66.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub->accelerate) (2024.6.2)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
            "Installing collected packages: nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, bitsandbytes, accelerate\n",
            "Successfully installed accelerate-0.32.0 bitsandbytes-0.43.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.20.5 nvidia-nvjitlink-cu12-12.5.82 nvidia-nvtx-cu12-12.1.105\n"
          ]
        }
      ],
      "source": [
        "!pip install accelerate bitsandbytes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "SGhRFOJjsr3R"
      },
      "outputs": [],
      "source": [
        "from typing import Union\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "import transformers\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "import accelerate\n",
        "\n",
        "os.environ['CUDA_VISIBLE_DEVICES'] = '0'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CldppYju6Wxr"
      },
      "source": [
        "## Data loading"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JpPFcV9D6X4k"
      },
      "outputs": [],
      "source": [
        "ROOT_DATA_RAW = '/content'\n",
        "HUMAN_JSON_FILE_NAME = 'human.jsonl'\n",
        "HUMAN_JSON_PATH = f'{ROOT_DATA_RAW}/{HUMAN_JSON_FILE_NAME}'\n",
        "BIGSCIENCE_PATH = f'{ROOT_DATA_RAW}/bigscience-bloomz-7b1.jsonl'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WZNJaf--7NGM"
      },
      "outputs": [],
      "source": [
        "df = pd.read_json(path_or_buf=HUMAN_JSON_PATH, lines=True)\n",
        "df['text_index'] = df.index\n",
        "df['is_llm'] = 0\n",
        "temp_df = pd.read_json(path_or_buf=HUMAN_JSON_PATH, lines=True)\n",
        "temp_df['text_index'] = df.index\n",
        "temp_df['is_llm'] = 1\n",
        "\n",
        "df = pd.concat([df, temp_df], ignore_index=True)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tbDXu4kKsu5a"
      },
      "source": [
        "## Metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "Dbt_YWh_spki"
      },
      "outputs": [],
      "source": [
        "ce_loss_fn = torch.nn.CrossEntropyLoss(reduction=\"none\")\n",
        "softmax_fn = torch.nn.Softmax(dim=-1)\n",
        "\n",
        "def perplexity(encoding: transformers.BatchEncoding,\n",
        "               logits: torch.Tensor,\n",
        "               median: bool = False,\n",
        "               temperature: float = 1.0):\n",
        "    shifted_logits = logits[..., :-1, :].contiguous() / temperature\n",
        "    shifted_labels = encoding.input_ids[..., 1:].contiguous()\n",
        "    shifted_attention_mask = encoding.attention_mask[..., 1:].contiguous()\n",
        "\n",
        "    if median:\n",
        "        ce_nan = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels).\n",
        "                  masked_fill(~shifted_attention_mask.bool(), float(\"nan\")))\n",
        "        ppl = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
        "\n",
        "    else:\n",
        "        ppl = (ce_loss_fn(shifted_logits.transpose(1, 2), shifted_labels) *\n",
        "               shifted_attention_mask).sum(1) / shifted_attention_mask.sum(1)\n",
        "        ppl = ppl.to(\"cpu\").float().numpy()\n",
        "\n",
        "    return ppl\n",
        "\n",
        "\n",
        "def entropy(p_logits: torch.Tensor,\n",
        "            q_logits: torch.Tensor,\n",
        "            encoding: transformers.BatchEncoding,\n",
        "            pad_token_id: int,\n",
        "            median: bool = False,\n",
        "            sample_p: bool = False,\n",
        "            temperature: float = 1.0):\n",
        "    vocab_size = p_logits.shape[-1]\n",
        "    total_tokens_available = q_logits.shape[-2]\n",
        "    p_scores, q_scores = p_logits / temperature, q_logits / temperature\n",
        "\n",
        "    p_proba = softmax_fn(p_scores).view(-1, vocab_size)\n",
        "\n",
        "    if sample_p:\n",
        "        p_proba = torch.multinomial(p_proba.view(-1, vocab_size), replacement=True, num_samples=1).view(-1)\n",
        "\n",
        "    q_scores = q_scores.view(-1, vocab_size)\n",
        "\n",
        "    ce = ce_loss_fn(input=q_scores, target=p_proba).view(-1, total_tokens_available)\n",
        "    padding_mask = (encoding.input_ids != pad_token_id).type(torch.uint8)\n",
        "\n",
        "    if median:\n",
        "        ce_nan = ce.masked_fill(~padding_mask.bool(), float(\"nan\"))\n",
        "        agg_ce = np.nanmedian(ce_nan.cpu().float().numpy(), 1)\n",
        "    else:\n",
        "        agg_ce = (((ce * padding_mask).sum(1) / padding_mask.sum(1)).to(\"cpu\").float().numpy())\n",
        "\n",
        "    return agg_ce"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "ODneWWuLs-pJ"
      },
      "outputs": [],
      "source": [
        "def assert_tokenizer_consistency(model_id_1, model_id_2):\n",
        "    identical_tokenizers = (\n",
        "            AutoTokenizer.from_pretrained(model_id_1).vocab\n",
        "            == AutoTokenizer.from_pretrained(model_id_2).vocab\n",
        "    )\n",
        "    if not identical_tokenizers:\n",
        "        raise ValueError(f\"Tokenizers are not identical for {model_id_1} and {model_id_2}.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "cCDaku477ov7"
      },
      "outputs": [],
      "source": [
        "OBSERVER = \"mistralai/Mistral-7B-v0.1\"\n",
        "PERFORMER = \"mistralai/Mistral-7B-Instruct-v0.1\"\n",
        "DEVICE_1 = \"cuda:0\" if torch.cuda.is_available() else \"cpu\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "212lybJmtMPM"
      },
      "source": [
        "## Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "LVvYnQAMsaBM"
      },
      "outputs": [],
      "source": [
        "\n",
        "torch.set_grad_enabled(False)\n",
        "\n",
        "huggingface_config = {\n",
        "    # Only required for private models from Huggingface (e.g. LLaMA models)\n",
        "    \"TOKEN\": os.environ.get(\"HF_TOKEN\", None)\n",
        "}\n",
        "# selected using Falcon-7B and Falcon-7B-Instruct at bfloat16\n",
        "BINOCULARS_ACCURACY_THRESHOLD = 0.9015310749276843  # optimized for f1-score\n",
        "BINOCULARS_FPR_THRESHOLD = 0.8536432310785527  # optimized for low-fpr [chosen at 0.01%]\n",
        "\n",
        "DEVICE_2 = \"cuda:1\" if torch.cuda.device_count() > 1 else DEVICE_1\n",
        "\n",
        "\n",
        "class Binoculars(object):\n",
        "    def __init__(self,\n",
        "                 observer_name_or_path: str = \"mistralai/Mistral-7B-v0.1\",\n",
        "                 performer_name_or_path: str = \"mistralai/Mistral-7B-Instruct-v0.1\",\n",
        "                 use_bfloat16: bool = True,\n",
        "                 max_token_observed: int = 512,\n",
        "                 mode: str = \"low-fpr\",\n",
        "                 ) -> None:\n",
        "        assert_tokenizer_consistency(observer_name_or_path, performer_name_or_path)\n",
        "\n",
        "        self.change_mode(mode)\n",
        "        self.observer_model =  AutoModelForCausalLM.from_pretrained(observer_name_or_path,\n",
        "                                                                   device_map={\"\": DEVICE_1},\n",
        "                                                                   trust_remote_code=True,\n",
        "                                                                   token=huggingface_config[\"TOKEN\"],\n",
        "                                                                   load_in_4bit=True\n",
        "                                                                   )\n",
        "        self.performer_model = AutoModelForCausalLM.from_pretrained(performer_name_or_path,\n",
        "                                                                    device_map={\"\": DEVICE_2},\n",
        "                                                                    trust_remote_code=True,\n",
        "                                                                    load_in_4bit=True,\n",
        "                                                                    token=huggingface_config[\"TOKEN\"]\n",
        "                                                                    )\n",
        "        self.observer_model.eval()\n",
        "        self.performer_model.eval()\n",
        "\n",
        "        self.tokenizer = AutoTokenizer.from_pretrained(observer_name_or_path)\n",
        "        if not self.tokenizer.pad_token:\n",
        "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
        "        self.max_token_observed = max_token_observed\n",
        "\n",
        "    def change_mode(self, mode: str) -> None:\n",
        "        if mode == \"low-fpr\":\n",
        "            self.threshold = BINOCULARS_FPR_THRESHOLD\n",
        "        elif mode == \"accuracy\":\n",
        "            self.threshold = BINOCULARS_ACCURACY_THRESHOLD\n",
        "        else:\n",
        "            raise ValueError(f\"Invalid mode: {mode}\")\n",
        "\n",
        "    def _tokenize(self, batch: list[str]) -> transformers.BatchEncoding:\n",
        "        batch_size = len(batch)\n",
        "        encodings = self.tokenizer(\n",
        "            batch,\n",
        "            return_tensors=\"pt\",\n",
        "            padding=\"longest\" if batch_size > 1 else False,\n",
        "            truncation=True,\n",
        "            max_length=self.max_token_observed,\n",
        "            return_token_type_ids=False).to(self.observer_model.device)\n",
        "        return encodings\n",
        "\n",
        "    @torch.inference_mode()\n",
        "    def _get_logits(self, encodings: transformers.BatchEncoding) -> torch.Tensor:\n",
        "        observer_logits = self.observer_model(**encodings.to(DEVICE_1)).logits\n",
        "        performer_logits = self.performer_model(**encodings.to(DEVICE_2)).logits\n",
        "        if DEVICE_1 != \"cpu\":\n",
        "            torch.cuda.synchronize()\n",
        "        return observer_logits, performer_logits\n",
        "\n",
        "    def compute_score(self, input_text: Union[list[str], str]) -> Union[float, list[float]]:\n",
        "        batch = [input_text] if isinstance(input_text, str) else input_text\n",
        "        encodings = self._tokenize(batch)\n",
        "        observer_logits, performer_logits = self._get_logits(encodings)\n",
        "        ppl = perplexity(encodings, performer_logits)\n",
        "        x_ppl = entropy(observer_logits.to(DEVICE_1), performer_logits.to(DEVICE_1),\n",
        "                        encodings.to(DEVICE_1), self.tokenizer.pad_token_id)\n",
        "        binoculars_scores = ppl / x_ppl\n",
        "        binoculars_scores = binoculars_scores.tolist()\n",
        "        return binoculars_scores[0] if isinstance(input_text, str) else binoculars_scores\n",
        "\n",
        "    def predict(self, input_text: Union[list[str], str]) -> Union[list[str], str]:\n",
        "        binoculars_scores = np.array(self.compute_score(input_text))\n",
        "        #pred = np.where(binoculars_scores < self.threshold,\n",
        "        #                \"Most likely AI-generated\",\n",
        "        #                \"Most likely human-generated\"\n",
        "        #                ).tolist()\n",
        "        return binoculars_scores.tolist()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "bQVNS0eJsozR"
      },
      "outputs": [],
      "source": [
        "MINIMUM_TOKENS = 64\n",
        "\n",
        "def count_tokens(text, tokenizer):\n",
        "    return len(tokenizer(text).input_ids)\n",
        "\n",
        "\n",
        "def run_detector(model, input_str, tokenizer):\n",
        "    #if count_tokens(input_str, tokenizer) < MINIMUM_TOKENS:\n",
        "    #    return f\"Too short length. Need minimum {MINIMUM_TOKENS} tokens to run Binoculars.\"\n",
        "    return f\"{model.predict(input_str)}\"\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87,
          "referenced_widgets": [
            "0445d86bbdbe40f1845f02b2ad877af0",
            "fb7323e776d9449baf96a87fb861f97e",
            "69088cb74b4b41339f956ecd53c612ab",
            "dc6431a293ad4aa1b94826465cc0e4e8",
            "b183f1573fe64c118977393139b1ecdd",
            "ca4b2ae0d53c4ef293e93357ef5b9550",
            "0038f892e19c43afaa73c2a67cda9e4a",
            "cdda35bd945e494ca2d2c785aa478ee7",
            "c531bb8d34574d7d9bb9bd46f5171308",
            "fe75d5dfc3574b5c9f0dcb910d607098",
            "da24211b8ebd4f8ebde527de1bc452a2"
          ]
        },
        "id": "mjqyVnh2wo1V",
        "outputId": "54cc8bd9-4ee7-4098-f74a-a639ee83841b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The `load_in_4bit` and `load_in_8bit` arguments are deprecated and will be removed in the future versions. Please, pass a `BitsAndBytesConfig` object in `quantization_config` argument instead.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0445d86bbdbe40f1845f02b2ad877af0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model = Binoculars()\n",
        "tokenizer = model.tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Rjqx-A_q8hNM"
      },
      "outputs": [],
      "source": [
        "def get_binoculars_score(text):\n",
        "    return run_detector(model, text, tokenizer)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 93
        },
        "id": "djoBu6VEt04F",
        "outputId": "c64cc7c8-e17b-4978-f3c9-2bf5437883b0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/bitsandbytes/nn/modules.py:426: UserWarning: Input type into Linear4bit is torch.float16, but bnb_4bit_compute_dtype=torch.float32 (default). This will lead to slow inference or training speed.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'Most likely human-generated'"
            ]
          },
          "execution_count": 9,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "test_str = 'Dr. Capy Cosmos, a capybara unlike any other, astounded the scientific community with his groundbreaking research in astrophysics. With his keen sense of observation and unparalleled ability to interpret cosmic data, he uncovered new insights into the mysteries of black holes and the origins of the universe. As hepeered through telescopes with his large, round eyes, fellow researchers often remarked that it seemedas if the stars themselves whispered their secrets directly to him. Dr. Cosmos not only became a beaconof inspiration to aspiring scientists but also proved that intellect and innovation can be found in the mostunexpected of creatures.'\n",
        "#test_str = '1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,41,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,4,1,2,3,41,2,3,41,2,3,41,2,3,41,2,3,4,1,2,3,41,2,3,4'\n",
        "#test_str = \"Spending two weeks in Rome gives you plenty of time to explore its rich history, vibrant culture, and delicious cuisine. Here is a comprehensive itinerary to make the most of your stay:Week 1Day 1: Historical LandmarksColosseum – Tour this iconic ancient amphitheater.Roman Forum – Walk through the heart of ancient Rome.Palatine Hill – Explore the birthplace of Rome.Day 2: Vatican CitySt. Peter's Basilica – Visit the largest church in the world.Sistine Chapel – Marvel at Michelangelo’s masterpiece.Vatican Museums – Explore vast collections of art and history.Day 3: Classical RomePantheon – See this well-preserved Roman temple.Piazza Navona – Enjoy the baroque architecture and fountains.Campo de' Fiori – Experience the vibrant market.\"\n",
        "#test_str = \"shinimahuinq\"\n",
        "run_detector(model, test_str, tokenizer)\n",
        "\n",
        "df['binoculars_score'] = df['text'].apply(get_binoculars_score)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0038f892e19c43afaa73c2a67cda9e4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "0445d86bbdbe40f1845f02b2ad877af0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_fb7323e776d9449baf96a87fb861f97e",
              "IPY_MODEL_69088cb74b4b41339f956ecd53c612ab",
              "IPY_MODEL_dc6431a293ad4aa1b94826465cc0e4e8"
            ],
            "layout": "IPY_MODEL_b183f1573fe64c118977393139b1ecdd"
          }
        },
        "69088cb74b4b41339f956ecd53c612ab": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_cdda35bd945e494ca2d2c785aa478ee7",
            "max": 2,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_c531bb8d34574d7d9bb9bd46f5171308",
            "value": 1
          }
        },
        "b183f1573fe64c118977393139b1ecdd": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "c531bb8d34574d7d9bb9bd46f5171308": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "ca4b2ae0d53c4ef293e93357ef5b9550": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cdda35bd945e494ca2d2c785aa478ee7": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "da24211b8ebd4f8ebde527de1bc452a2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "dc6431a293ad4aa1b94826465cc0e4e8": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_fe75d5dfc3574b5c9f0dcb910d607098",
            "placeholder": "​",
            "style": "IPY_MODEL_da24211b8ebd4f8ebde527de1bc452a2",
            "value": " 1/2 [00:59&lt;00:59, 59.18s/it]"
          }
        },
        "fb7323e776d9449baf96a87fb861f97e": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_ca4b2ae0d53c4ef293e93357ef5b9550",
            "placeholder": "​",
            "style": "IPY_MODEL_0038f892e19c43afaa73c2a67cda9e4a",
            "value": "Loading checkpoint shards:  50%"
          }
        },
        "fe75d5dfc3574b5c9f0dcb910d607098": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
