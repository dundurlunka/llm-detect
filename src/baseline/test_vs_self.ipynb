{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import json\n",
    "\n",
    "from tqdm import tqdm\n",
    "from torch import nn\n",
    "from transformers import AutoTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from timeit import default_timer as timer\n",
    "from os import walk\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score, roc_curve, auc, brier_score_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_DATA = '../../data'\n",
    "HUMAN_JSON_FILE_NAME = 'human.jsonl'\n",
    "HUMAN_JSON_PATH = f'{ROOT_DATA}/raw/{HUMAN_JSON_FILE_NAME}'\n",
    "MODELS_JSON_FOLDER_PATH = f'{ROOT_DATA}/raw/machines'\n",
    "BASELINE_MODELS_FOLDER_PATH = f'{ROOT_DATA}/baseline/models'\n",
    "BASELINE_MODELS_RESULTS_FOLDER_PATH = f'{ROOT_DATA}/baseline/results'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 32\n",
    "LSTM_UNITS = 256\n",
    "LSTM_LAYERS = 5\n",
    "EMBEDDING_SIZE = 512"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEST_SET_FRACTION = 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json(path_or_buf=HUMAN_JSON_PATH, lines=True)\n",
    "df['text_index'] = df.index\n",
    "df['is_llm'] = 0\n",
    "df['dataset_name'] = Path(HUMAN_JSON_FILE_NAME).stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "dir_path, dir_names, file_names = next(walk(MODELS_JSON_FOLDER_PATH))\n",
    "\n",
    "for file_name in file_names:\n",
    "    temp_df = pd.read_json(path_or_buf=f'{MODELS_JSON_FOLDER_PATH}/{file_name}', lines=True)\n",
    "    temp_df['text_index'] = temp_df.index\n",
    "    temp_df['is_llm'] = 1\n",
    "    temp_df['dataset_name'] = Path(file_name).stem\n",
    "\n",
    "    df = pd.concat([df, temp_df], ignore_index=True)\n",
    "\n",
    "df.drop(labels=['id'], inplace=True, axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = 'distilbert-base-cased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (843 > 512). Running this sequence through the model will result in indexing errors\n"
     ]
    }
   ],
   "source": [
    "df['tokenized_text'] = tokenizer(list(df['text'].to_list()))['input_ids']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_size, hidden_size, layers_num, device, output_size=1, dropout=0):\n",
    "        super().__init__()\n",
    "\n",
    "        self.vocab_size = vocab_size\n",
    "        self.embedding_size = embedding_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.layers_num = layers_num\n",
    "        self.output_size= output_size\n",
    "        self.dropout = dropout\n",
    "        self.device = device\n",
    "\n",
    "        self.embed = nn.Embedding(self.vocab_size, self.embedding_size, device=self.device)\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=self.embedding_size,\n",
    "            hidden_size=self.hidden_size,\n",
    "            num_layers=self.layers_num,\n",
    "            batch_first=True,\n",
    "            dropout=self.dropout,\n",
    "            device=self.device\n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(\n",
    "            self.hidden_size,\n",
    "            self.output_size\n",
    "        )\n",
    "\n",
    "    def forward(self, X, lengths):\n",
    "        embeddings = self.embed(X)\n",
    "\n",
    "        seq_output, (h_n, c_n) = self.lstm(embeddings)\n",
    "\n",
    "        out = seq_output.sum(dim=1).div(lengths.float().unsqueeze(dim=1))\n",
    "        logits = self.fc(out)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.X.size\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return (\n",
    "            self.X[index],\n",
    "            self.y[index]\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(batch):\n",
    "  # We want to sort the batch by seq length,\n",
    "  # in order to make the computation more efficient\n",
    "  batch = sorted(batch, key=lambda x: len(x[0]), reverse=True)\n",
    "\n",
    "  inputs = [torch.LongTensor(x[0]).to(device) for x in batch]\n",
    "  padded_input = nn.utils.rnn.pad_sequence(inputs, batch_first=True)\n",
    "\n",
    "  lengths = torch.LongTensor([len(x[0]) for x in batch]).to(device)\n",
    "\n",
    "  y = torch.FloatTensor(np.array([x[1] for x in batch])).reshape(-1, 1).to(device)\n",
    "\n",
    "  return padded_input, lengths, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(y_true, y_hat):\n",
    "    correct_pred = torch.eq(torch.sigmoid(y_hat).round(), y_true).sum().item()\n",
    "    return (correct_pred / len(y_hat)) * 100\n",
    "\n",
    "def calculate_f1(y_true, y_hat):\n",
    "    y_pred = torch.sigmoid(y_hat).round()\n",
    "    return f1_score(y_true, y_pred)\n",
    "\n",
    "def calculate_brier(y_true, y_hat):\n",
    "    y_prob = torch.sigmoid(y_hat)\n",
    "    return brier_score_loss(y_true, y_prob)\n",
    "\n",
    "def calculate_auc(y_true, y_hat):\n",
    "    y_prob = torch.sigmoid(y_hat)\n",
    "\n",
    "    false_positive_rates, true_positive_rates, _ = roc_curve(y_true, y_prob)\n",
    "    roc_auc = auc(false_positive_rates, true_positive_rates)\n",
    "\n",
    "    return roc_auc, false_positive_rates, true_positive_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(model, dataloader, loss_fn, device):\n",
    "    model.eval()\n",
    "\n",
    "    all_y_true = []\n",
    "    all_y_hat = []\n",
    "\n",
    "    test_loss = 0\n",
    "    steps = 0\n",
    "\n",
    "    with torch.inference_mode():\n",
    "        for X, lengths, y in dataloader:\n",
    "\n",
    "            X, y = X.to(device), y.to(device)\n",
    "\n",
    "            y_hat = model(X, lengths)\n",
    "\n",
    "            all_y_true.extend(y)\n",
    "            all_y_hat.extend(y_hat)\n",
    "\n",
    "            loss = loss_fn(y_hat, y)\n",
    "            test_loss += loss.item()\n",
    "\n",
    "            steps += 1\n",
    "\n",
    "        all_y_true = torch.FloatTensor(all_y_true)\n",
    "        all_y_hat = torch.FloatTensor(all_y_hat)\n",
    "\n",
    "        test_accuracy = calculate_accuracy(all_y_true, all_y_hat)\n",
    "        test_f1 = calculate_f1(all_y_true, all_y_hat)\n",
    "        test_brier = calculate_brier(all_y_true, all_y_hat)\n",
    "        test_auc_tuple = calculate_auc(all_y_true, all_y_hat)\n",
    "\n",
    "    return test_loss / steps, test_accuracy, test_f1, test_brier, test_auc_tuple\n",
    "\n",
    "def test(model, loss_fn, device, test_df):\n",
    "    test_dataset_full = TextDataset(test_df['tokenized_text'], test_df['is_llm'])\n",
    "    test_dataloader_full = DataLoader(\n",
    "            test_dataset_full,\n",
    "            batch_size=BATCH_SIZE,\n",
    "            shuffle=False,\n",
    "            drop_last=False,\n",
    "            collate_fn=collate_fn\n",
    "        )\n",
    "\n",
    "    start_time = timer()\n",
    "\n",
    "    test_loss, test_acc, test_f1, test_brier, test_auc_tuple = test_step(\n",
    "        model,\n",
    "        test_dataloader_full,\n",
    "        loss_fn,\n",
    "        device\n",
    "    )\n",
    "\n",
    "    end_time = timer()\n",
    "\n",
    "    results_formatted = {\n",
    "        \"test_loss\": test_loss,\n",
    "        \"test_acc\": test_acc,\n",
    "        \"test_f1\": test_f1,\n",
    "        \"test_brier\": test_brier,\n",
    "        \"test_auc_tuple\": test_auc_tuple\n",
    "    }\n",
    "        \n",
    "    print(\n",
    "        f\"test_loss: {test_loss:.4f} | \"\n",
    "        f\"test_acc: {test_acc:.4f} | \"\n",
    "        f\"test_f1: {test_f1:.4f} | \"\n",
    "        f\"test_brier: {test_brier:.4f} | \"\n",
    "        f\"time: {(end_time-start_time):.4f}\"\n",
    "    )\n",
    "    \n",
    "    return results_formatted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "human_df = df.loc[df['is_llm'] == 0]\n",
    "human_train_df, human_test_df = train_test_split(human_df, test_size=TEST_SET_FRACTION, random_state=69)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class json_serialize(json.JSONEncoder):\n",
    "    def default(self, obj):\n",
    "        if isinstance(obj, np.integer):\n",
    "            return int(obj)\n",
    "        if isinstance(obj, np.floating):\n",
    "            return float(obj)\n",
    "        if isinstance(obj, np.ndarray):\n",
    "            return obj.tolist()\n",
    "        return json.JSONEncoder.default(self, obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing alpaca-7b against itself...\n",
      "test_loss: 0.3260 | test_acc: 94.4954 | test_f1: 0.9439 | test_brier: 0.0480 | time: 21.0661\n",
      "Finished testing alpaca-7b against itself.\n",
      "Testing bigscience-bloomz-7b1 against itself...\n",
      "test_loss: 0.5942 | test_acc: 84.7095 | test_f1: 0.8270 | test_brier: 0.1172 | time: 22.2123\n",
      "Finished testing bigscience-bloomz-7b1 against itself.\n",
      "Testing chavinlo-alpaca-13b against itself...\n",
      "test_loss: 0.2721 | test_acc: 96.1774 | test_f1: 0.9610 | test_brier: 0.0340 | time: 23.1426\n",
      "Finished testing chavinlo-alpaca-13b against itself.\n",
      "Testing gemini-pro against itself...\n",
      "test_loss: 0.1784 | test_acc: 93.8838 | test_f1: 0.9373 | test_brier: 0.0480 | time: 26.7833\n",
      "Finished testing gemini-pro against itself.\n",
      "Testing gpt-3.5-turbo-0125 against itself...\n",
      "test_loss: 0.3837 | test_acc: 92.0489 | test_f1: 0.9205 | test_brier: 0.0624 | time: 24.0087\n",
      "Finished testing gpt-3.5-turbo-0125 against itself.\n",
      "Testing gpt-4-turbo-preview against itself...\n",
      "test_loss: 0.0531 | test_acc: 98.1651 | test_f1: 0.9818 | test_brier: 0.0130 | time: 27.1066\n",
      "Finished testing gpt-4-turbo-preview against itself.\n",
      "Testing meta-llama-llama-2-70b-chat-hf against itself...\n",
      "test_loss: 0.1051 | test_acc: 96.0245 | test_f1: 0.9596 | test_brier: 0.0305 | time: 27.6227\n",
      "Finished testing meta-llama-llama-2-70b-chat-hf against itself.\n",
      "Testing meta-llama-llama-2-7b-chat-hf against itself...\n",
      "test_loss: 0.4106 | test_acc: 91.7431 | test_f1: 0.9194 | test_brier: 0.0629 | time: 25.9927\n",
      "Finished testing meta-llama-llama-2-7b-chat-hf against itself.\n",
      "Testing mistralai-mistral-7b-instruct-v0.2 against itself...\n",
      "test_loss: 0.2714 | test_acc: 90.5199 | test_f1: 0.9122 | test_brier: 0.0751 | time: 28.1703\n",
      "Finished testing mistralai-mistral-7b-instruct-v0.2 against itself.\n",
      "Testing mistralai-mixtral-8x7b-instruct-v0.1 against itself...\n",
      "test_loss: 0.2630 | test_acc: 91.1315 | test_f1: 0.9094 | test_brier: 0.0710 | time: 28.2614\n",
      "Finished testing mistralai-mixtral-8x7b-instruct-v0.1 against itself.\n",
      "Testing qwen-qwen1.5-72b-chat-8bit against itself...\n",
      "test_loss: 0.2159 | test_acc: 95.5657 | test_f1: 0.9557 | test_brier: 0.0370 | time: 22.4902\n",
      "Finished testing qwen-qwen1.5-72b-chat-8bit against itself.\n",
      "Testing text-bison-002 against itself...\n",
      "test_loss: 4.6849 | test_acc: 93.7309 | test_f1: 0.9358 | test_brier: 0.0524 | time: 28.4618\n",
      "Finished testing text-bison-002 against itself.\n",
      "Testing vicgalle-gpt2-open-instruct-v1 against itself...\n",
      "test_loss: 0.8774 | test_acc: 84.2508 | test_f1: 0.8177 | test_brier: 0.1381 | time: 24.7493\n",
      "Finished testing vicgalle-gpt2-open-instruct-v1 against itself.\n"
     ]
    }
   ],
   "source": [
    "final_results = []\n",
    "\n",
    "for llm_name in df['dataset_name'].unique():\n",
    "    if llm_name == Path(HUMAN_JSON_FILE_NAME).stem:\n",
    "        continue\n",
    "    \n",
    "    llm_df = df.loc[df['dataset_name'] == llm_name]\n",
    "    llm_df_train, llm_df_test = train_test_split(llm_df, test_size=TEST_SET_FRACTION, random_state=69)\n",
    "    test_df = pd.concat([human_test_df, llm_df_test], ignore_index=True)    \n",
    "\n",
    "    model_path = f'{BASELINE_MODELS_FOLDER_PATH}/{llm_name}.pt'\n",
    "\n",
    "    model = RNN(tokenizer.vocab_size, EMBEDDING_SIZE, LSTM_UNITS, LSTM_LAYERS, device, dropout=0.6).to(device)\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    print(f'Testing {llm_name} against itself...')\n",
    "\n",
    "    results_formatted = test(model, loss_fn, device, test_df)\n",
    "\n",
    "    with open(f'{BASELINE_MODELS_RESULTS_FOLDER_PATH}/{llm_name}_test_metrics.json', 'w', encoding='utf-8') as f:\n",
    "        json.dump(results_formatted, f, ensure_ascii=False, indent=4, cls=json_serialize)\n",
    "\n",
    "    print(f'Finished testing {llm_name} against itself.')\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-detect",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
